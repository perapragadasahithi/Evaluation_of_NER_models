{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ner(f,f1,a1):\r\n",
    "    #loading the required libraries\r\n",
    "    from simpletransformers.ner import NERModel\r\n",
    "    import pandas as pd\r\n",
    "    import logging\r\n",
    "    import re\r\n",
    "    from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\r\n",
    "    from transformers import pipeline\r\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification\r\n",
    "    import openpyxl\r\n",
    "    from openpyxl import load_workbook\r\n",
    "    logging.basicConfig(level=logging.DEBUG)\r\n",
    "    transformers_logger = logging.getLogger('transformers')\r\n",
    "    transformers_logger.setLevel(logging.WARNING)\r\n",
    "\r\n",
    "    \r\n",
    "   #reading  text from input files\r\n",
    "    file=\"/host/Sahithi/myenv/input/input/\"+f1\r\n",
    "    with open(file, \"r\") as cur:\r\n",
    "        text=cur.read()\r\n",
    "\r\n",
    "    #preprocessing of data\r\n",
    "    exceptions = {\r\n",
    "            r'\\d+mg': r'\\g<0> |\\g<1>',\r\n",
    "            r'\\d+:\\d+': r'\\g<0>',\r\n",
    "            r'[\\,\\-\\(\\)]': r' \\g<0> ',\r\n",
    "            r'(?<!\\d)\\d+\\.\\d+(?!\\d)': r'\\g<0>',\r\n",
    "            r'\\d+/\\d+': r'\\g<0>',\r\n",
    "            r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}': r'\\g<0>',\r\n",
    "            r'[A-Za-z]\\d': r'\\g<0>',\r\n",
    "            r'[^\\.\\s]\\.[^\\.\\s]': r'\\g<0>',\r\n",
    "            r'\\d+\\.\\d+':r'\\g<0>',\r\n",
    "        }\r\n",
    "    tokenizer = TreebankWordTokenizer()\r\n",
    "    tokens = tokenizer.tokenize(text)\r\n",
    "    processed_tokens = []\r\n",
    "\r\n",
    "    a = [\"COVID-19\", \"4/5Unable\", \"workstation:<ID\", \"patientÃ‚\\x92s\"]\r\n",
    "    # List to store processed sentences\r\n",
    "    sentences = [] \r\n",
    "\r\n",
    "    for token in tokens:\r\n",
    "        if token in a:\r\n",
    "            processed_tokens.append(token)\r\n",
    "        elif token not in exceptions and (\":\" in token or \",\" in token or \"/\" in token or \".\" in token or \"<\" in token or \">\" in token or \"-\" in token):\r\n",
    "            if re.match(r'\\d+/\\d+', token) or re.match(r'\\d+:\\d+', token):\r\n",
    "                processed_tokens.append(token)\r\n",
    "            elif re.match(r'(?<!\\d)\\d+\\.\\d+(?!\\d)', token):\r\n",
    "                processed_tokens.append(token)\r\n",
    "            else:\r\n",
    "                split_tokens = re.split(r'(:|,|/|\\.|<|>|-)', token)\r\n",
    "                processed_tokens.extend([t for t in split_tokens if t])\r\n",
    "        elif token in exceptions:\r\n",
    "            processed_tokens.append(exceptions[token])\r\n",
    "        else:\r\n",
    "            processed_tokens.append(token)\r\n",
    "\r\n",
    "    # Iterate over sentences and append processed tokens\r\n",
    "    current_sentence =\"\"\r\n",
    "    sentences=\"\"\r\n",
    "    for token in processed_tokens:\r\n",
    "        current_sentence=current_sentence+\" \"+token\r\n",
    "   \r\n",
    "        if token.endswith('.') or token.endswith('!') or token.endswith('?'):\r\n",
    "            sentences=sentences+\" \"+current_sentence\r\n",
    "            current_sentence=\"\"\r\n",
    "\r\n",
    "    # Append the remaining tokens as the last sentence\r\n",
    "    if current_sentence:\r\n",
    "        sentences=sentences+current_sentence\r\n",
    "    c=[]\r\n",
    "    sentences1=sentences.replace('REMITTING MULTIPLE SCLEROSIS','remitting multiple sclerosis')\r\n",
    "    \r\n",
    "    #splitting the total sentences with maximum length as 350(it can be userdefined )\r\n",
    "    def process_input(s, max_length):\r\n",
    "        if len(s) <= max_length:\r\n",
    "            c.append(s)\r\n",
    "        else:\r\n",
    "            while len(s) > 0:\r\n",
    "                chunk = s[:max_length] \r\n",
    "                c.append(chunk)\r\n",
    "                s = s[max_length:]\r\n",
    "\r\n",
    "\r\n",
    "    max_length = 350\r\n",
    "    process_input(sentences1, max_length)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    # Annotated file for each document\r\n",
    "    a2=\"/host/Sahithi/myenv/output_entity/\"+a1\r\n",
    "    df = pd.read_csv(a2, delimiter='\\n', header=None, quoting=3)\r\n",
    "    annotated_text = df[0].tolist() \r\n",
    "\r\n",
    "    # making a list 'a'  with the \r\n",
    "    a = []\r\n",
    "    for i in annotated_text:\r\n",
    "        b = i.split(' ')\r\n",
    "        a.append(b)     \r\n",
    "\r\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\r\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")\r\n",
    "    pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\r\n",
    "    x=[]\r\n",
    "    p=[]\r\n",
    "\r\n",
    "    #c is the chunks of the data with max limit 350\r\n",
    "    for i in c: \r\n",
    "        p.extend(pipe(i))\r\n",
    "    \r\n",
    "    #combining predictions dictioneries of the chunks into a list\r\n",
    "    predicted_tokens = []\r\n",
    "    for item in p:\r\n",
    "        for word in item['word'].split():\r\n",
    "            predicted_tokens.append([word,item['entity_group']])\r\n",
    "\r\n",
    "    #making list of annotated_tokens and labels\r\n",
    "    annotated_tokens = [[item[0],item[3]] for item in a] \r\n",
    "\r\n",
    "    #counting the number of annotated tokens of the corresponding conll files\r\n",
    "    annotated_num=0\r\n",
    "    for i in annotated_tokens:\r\n",
    "        if(i[1]!='O'):\r\n",
    "            annotated_num+=1 \r\n",
    "\r\n",
    "    #calculating the paramenters required for measuring performance\r\n",
    "    tp = []\r\n",
    "    tn = []\r\n",
    "    fp = []\r\n",
    "    fn = []\r\n",
    "\r\n",
    "    for item in predicted_tokens:\r\n",
    "        count = 0\r\n",
    "        for items in annotated_tokens:\r\n",
    "            if item[0] == items[0]  or items[0] in item[0].upper(): \r\n",
    "                count = 1\r\n",
    "                if (item[1] == '0') & (items[1]=='O'):\r\n",
    "                    tn.append([item,items])\r\n",
    "                    annotated_tokens.remove(items)\r\n",
    "                    break\r\n",
    "                elif (item[1] == 'DISEASE') & (items[1]!='O'):\r\n",
    "                    tp.append([item,items])\r\n",
    "                    annotated_tokens.remove(items)\r\n",
    "                    break\r\n",
    "                elif (item[1] == 'DISEASE') & (items[1]=='O'):\r\n",
    "                    fp.append([item,items])\r\n",
    "                    annotated_tokens.remove(items)\r\n",
    "                    break\r\n",
    "                elif (item[1] == '0') & (items[1]!='O'):\r\n",
    "                    fn.append([item,items])\r\n",
    "                    annotated_tokens.remove(items)\r\n",
    "                    break\r\n",
    "\r\n",
    "        if count == 0:\r\n",
    "            if item[1] == 'DISEASE':\r\n",
    "                fp.append([[item,[item[0],'O']]])\r\n",
    "            elif item[1] == '0':\r\n",
    "                tn.append([[item,[item[0],'O']]])\r\n",
    "    print(len(tp),len(fp),len(tn),len(fn))\r\n",
    "    \r\n",
    "    inscope=0\r\n",
    "    flag=False\r\n",
    "    anno=0\r\n",
    "    if(len(tp)==0 or len(fp)==0 or len(tn)==0 ):\r\n",
    "        print(\"somevalue is zer0\")\r\n",
    "          \r\n",
    "    else:\r\n",
    "         \r\n",
    "        flag=True\r\n",
    "\r\n",
    "        total=[]\r\n",
    "        [total.extend(l) for l in (tp,tn,fp,fn)]\r\n",
    "\r\n",
    "        token=[]\r\n",
    "        predict=[]\r\n",
    "        annot=[]\r\n",
    "        for i in total:\r\n",
    "            if(len(i)==2):\r\n",
    "                token.append(i[0][0])\r\n",
    "                predict.append(i[0][1])\r\n",
    "                annot.append(i[1][1])\r\n",
    "            elif(len(i)==1):\r\n",
    "                for j in i:\r\n",
    "                    token.append(j[0][0])\r\n",
    "                    predict.append(j[0][1])\r\n",
    "                    annot.append(j[1][1])\r\n",
    "\r\n",
    "        output=pd.DataFrame({\r\n",
    "                        'token':token,\r\n",
    "                        'predictions':predict,\r\n",
    "                        'annotated':annot,\r\n",
    "                    })\r\n",
    "        \r\n",
    "    #finding the number of inscope tokens \r\n",
    "        for i in total:  \r\n",
    "            if(len(i)==2):\r\n",
    "                if(i[0][1]!='0' and i[1][1]!='O'):\r\n",
    "                    inscope=inscope+1\r\n",
    "                    print(i)\r\n",
    "            elif len(i)==1:\r\n",
    "                for j in i:\r\n",
    "                    if(j[0][1]!='0' and j[1][1]!='O'):\r\n",
    "                        inscope=inscope+1\r\n",
    "                        print(j)\r\n",
    "       \r\n",
    "                        \r\n",
    "        print(\"inscope:\",inscope)\r\n",
    "        \r\n",
    "            \r\n",
    "        \r\n",
    "        \r\n",
    "    return len(token),annotated_num,inscope,flag,len(tp),len(tn),len(fp),len(fn)\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "import os\r\n",
    "files = os.listdir('/host/Sahithi/myenv/input/input/')\r\n",
    "an_files=os.listdir('/host/Sahithi/myenv/output_entity/')\r\n",
    "tokens=0\r\n",
    "annotated_tokens=0\r\n",
    "num_file=0\r\n",
    "inscope_tokens=0\r\n",
    "tp=0\r\n",
    "tn=0\r\n",
    "fp=0\r\n",
    "fn=0\r\n",
    "for an_file in an_files:\r\n",
    "    f=os.path.splitext(os.path.basename(an_file))[0]\r\n",
    "    for file in files:\r\n",
    "        f1=os.path.splitext(os.path.basename(file))[0]\r\n",
    "        if f1 in f:\r\n",
    "    #calling the function by passing the input file and its corresponding annotated conll file\r\n",
    "            t,an,ins,flag,t1,t2,t3,t4=ner(f1,file,an_file)\r\n",
    "            if flag==True:\r\n",
    "                tokens=tokens+t\r\n",
    "                annotated_tokens=annotated_tokens+an\r\n",
    "                tp=tp+t1\r\n",
    "                tn=tn+t2\r\n",
    "                fp=fp+t3\r\n",
    "                fn=fn+t4\r\n",
    "                inscope_tokens+=ins\r\n",
    "                num_file+=1\r\n",
    "                \r\n",
    "                \r\n",
    "            \r\n",
    "#finding the performance metrics for overall data          \r\n",
    "precision = tp/(tp+fp)\r\n",
    "recall = tp/(tp+fn)\r\n",
    "f1 = 2*(precision*recall)/(precision+recall)  \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"number of  tokens:\",tokens  )\r\n",
    "print(\"number of annotated tokens:\",annotated_tokens)\r\n",
    "print(\"inscope _tokens:\",inscope_tokens)\r\n",
    "print(\"true positives:\",tp)\r\n",
    "print(\"true negatives:\",tn)\r\n",
    "print(\"false positives:\",fp)\r\n",
    "print(\"false negatives:\",fn)\r\n",
    "\r\n",
    "\r\n",
    "print(\"precision:\",precision*100)\r\n",
    "print(\"recall:\",recall*100)\r\n",
    "print(\"f1 score:\",f1*100)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}