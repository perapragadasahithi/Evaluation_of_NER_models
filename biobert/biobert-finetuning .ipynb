{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#importing the required libraries\r\n",
    "from simpletransformers.ner import NERModel\r\n",
    "import pandas as pd\r\n",
    "import logging\r\n",
    "import re\r\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\r\n",
    "from transformers import pipeline\r\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\r\n",
    "import openpyxl\r\n",
    "from openpyxl import load_workbook\r\n",
    "logging.basicConfig(level=logging.DEBUG)\r\n",
    "transformers_logger = logging.getLogger('transformers')\r\n",
    "transformers_logger.setLevel(logging.WARNING)\r\n",
    "import datasets\r\n",
    "from datasets import Dataset\r\n",
    "import re\r\n",
    "from datasets import Features, Value, ClassLabel\r\n",
    "from datasets import load_dataset\r\n",
    "from datasets import concatenate_datasets, load_dataset\r\n",
    "import os\r\n",
    "\r\n",
    "#importing the conll file(test or train or validate)\r\n",
    "input_connl = '/host/Sahithi/myenv/an_validate_total.conll'\r\n",
    "\r\n",
    "#creating the dataset from the total annotated file\r\n",
    "def create_dataset_from_annotated_file(i):\r\n",
    "    input_file = i\r\n",
    "    sentences = []\r\n",
    "    current_sentence = []\r\n",
    "        with open(input_file, \"r\") as f:\r\n",
    "        for line in f:\r\n",
    "            line = line.strip()\r\n",
    "            if line :\r\n",
    "                if line.startswith('.'):\r\n",
    "                    word, _, _, label = line.split()\r\n",
    "                    current_sentence.append((word, label))\r\n",
    "                    if current_sentence:\r\n",
    "                        sentences.append(current_sentence)\r\n",
    "                    current_sentence = []\r\n",
    "                else:\r\n",
    "                    word, _, _, label = line.split()\r\n",
    "                    current_sentence.append((word, label))\r\n",
    "            else:\r\n",
    "\r\n",
    "                if current_sentence:\r\n",
    "                    sentences.append(current_sentence\r\n",
    "                current_sentence = []\r\n",
    "    if current_sentence:\r\n",
    "        sentences.append(current_sentence)\r\n",
    "    \r\n",
    "    c=0\r\n",
    "    conll_data = {\r\n",
    "\r\n",
    "        \"tokens\": [],\r\n",
    "        \"ner_tags\": []\r\n",
    "    }\r\n",
    "    s=0\r\n",
    "    for sent_id, sentence in enumerate(sentences):\r\n",
    "        tokens, labels = zip(*sentence)\r\n",
    "        s+=1\r\n",
    "        c+=1\r\n",
    "        conll_data = {\r\n",
    "    \r\n",
    "        \"tokens\": [],\r\n",
    "        \"ner_tags\": []\r\n",
    "    }\r\n",
    "        conll_data[\"tokens\"] = [list(tokens)]\r\n",
    "        conll_data[\"ner_tags\"] = [convert_to_int_labels(list(labels), labels_list)[0]]\r\n",
    "        class_names = ['O', 'I-RRMS', 'I-Disease_Subtype', 'B-RRMS', 'I-SPMS', 'B-Disease_Subtype', 'B-EDSS', 'B-SPMS', 'B-Relapse_relative_date', 'I-Relapse_Date', 'I-EDSS', 'B-Relapse_date', 'B-EDSS_Value', 'I-Temporal_history_of_relapse', 'I-PPMS', 'B-Temporal_history_of_relapse', 'I-PRMS', 'B-MS_no_subtype', 'B-EDSS_Date', 'I-EDSS_Value', 'B-Relapse_Date', 'I-MS_no_subtype', 'B-Relapse_Temporality', 'I-Relapse_Temporality', 'B-PPMS', 'I-Relapse_date', 'I-CIS', 'I-Relapse_year', 'I-EDSS_Date', 'I-Relapse_relative_date', 'B-Relapse_year', 'B-CIS', 'B-PRMS', 'B-Severity_Severe', 'B-Relapse_Severity', 'B-Functional_Disability', 'B-Severity_Mild', 'I-Severity_Severe']\r\n",
    "        ft = Features({\r\n",
    "                       'tokens': datasets.Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\r\n",
    "                       'ner_tags': datasets.Sequence(feature = ClassLabel(names=class_names), length = -1)})\r\n",
    "        if c==1:     \r\n",
    "            a = Dataset.from_dict(conll_data,features = ft)\r\n",
    "        else:\r\n",
    "            b = Dataset.from_dict(conll_data,features = ft)\r\n",
    "            assert b.features.type == a.features.type\r\n",
    "            a = concatenate_datasets([a,b])\r\n",
    "    return a\r\n",
    "                                     \r\n",
    "#conversion of string labels to integers\r\n",
    "def convert_to_int_labels(labels,labels_list):\r\n",
    "    label_to_int = {label: i for i, label in enumerate(labels_list)}\r\n",
    "    converted_ner_tags = []\r\n",
    "# Iterate through each example in the dataset and convert NER tags\r\n",
    "    converted_tags = [label_to_int[tag] for tag in labels]\r\n",
    "    converted_ner_tags.append(converted_tags)\r\n",
    "    return converted_ner_tags\r\n",
    " \r\n",
    "                                     \r\n",
    "#resultant dataset                                     \r\n",
    "test = create_dataset_from_annotated_file(input_connl)\r\n",
    " \r\n",
    "                                     \r\n",
    "#loading the model the model                                     \r\n",
    "from transformers import pipeline\r\n",
    "model_checkpoint = \"alvaroalon2/biobert_diseases_ner\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\r\n",
    "                                     \r\n",
    "                                     \r\n",
    "                                    \r\n",
    "ner_feature = test.features[\"ner_tags\"]\r\n",
    "label_names = ner_feature.feature.names\r\n",
    "                                     \r\n",
    "                                     \r\n",
    "#aligning the labels with the tokens                                   \r\n",
    "def align_labels_with_tokens(labels, word_ids):\r\n",
    "    new_labels = []\r\n",
    "    current_word = None\r\n",
    "    for word_id in word_ids:\r\n",
    "        if word_id != current_word:\r\n",
    "            # Start of a new word!\r\n",
    "            current_word = word_id\r\n",
    "            label = -100 if word_id is None else labels[word_id]\r\n",
    "            new_labels.append(label)\r\n",
    "        elif word_id is None:\r\n",
    "            # Special token\r\n",
    "            new_labels.append(-100)\r\n",
    "        else:\r\n",
    "            # Same word as previous token\r\n",
    "            label = labels[word_id]\r\n",
    "            # If the label is B-XXX we change it to I-XXX\r\n",
    "            if label % 2 == 1:\r\n",
    "                label += 1\r\n",
    "            new_labels.append(label)\r\n",
    "    return new_labels\r\n",
    "                                     \r\n",
    "                                     \r\n",
    "def tokenize_and_align_labels(examples):\r\n",
    "    tokenized_inputs = tokenizer(\r\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\r\n",
    "    )\r\n",
    "    all_labels = examples[\"ner_tags\"]\r\n",
    "    new_labels = []\r\n",
    "    for i, labels in enumerate(all_labels):\r\n",
    "        word_ids = tokenized_inputs.word_ids(i)\r\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\r\n",
    "\r\n",
    "    tokenized_inputs[\"labels\"] = new_labels\r\n",
    "    return tokenized_inputs\r\n",
    "                                     \r\n",
    "\r\n",
    "tokenized_datasets = test.map(\r\n",
    "    tokenize_and_align_labels,\r\n",
    "    batched=True,\r\n",
    "    remove_columns=test.column_names,\r\n",
    ")\r\n",
    "                                     \r\n",
    "                                     \r\n",
    "# Evaluation of the model over the complete dataset                                   \r\n",
    "import torch\r\n",
    "for batch in test_dataloader:\r\n",
    "    with torch.no_grad():\r\n",
    "        outputs = model(**batch)\r\n",
    "    predictions = outputs.logits.argmax(dim=-1)\r\n",
    "    labels = batch[\"labels\"]\r\n",
    "    # Necessary to pad predictions and labels for being gathered\r\n",
    "    predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\r\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\r\n",
    "    predictions_gathered = accelerator.gather(predictions)\r\n",
    "    labels_gathered = accelerator.gather(labels)\r\n",
    "    true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\r\n",
    "                                     \r\n",
    "    token_names=[]\r\n",
    "    true_predictions_extended=[]\r\n",
    "    true_labels_extended=[]\r\n",
    "                                     \r\n",
    "    for ids in batch['input_ids']:\r\n",
    "        tokens=tokenizer.batch_decode(ids,skip_special_tokens=True)\r\n",
    "        token_names.extend(tokens)\r\n",
    "        for i in range(token_names.count('')):\r\n",
    "            token_names.remove('')\r\n",
    "    for i in true_predictions:\r\n",
    "        true_predictions_extended.extend(i)\r\n",
    "    for i in true_labels:\r\n",
    "        true_labels_extended.extend(i)                               \r\n",
    "    if(len(token_names)==len(true_labels_extended)):\r\n",
    "        total=pd.DataFrame({\r\n",
    "            \"token_name\":token_names,\r\n",
    "            \"true_label\":true_labels_extended,\r\n",
    "            \"true_prediction\":true_predictions_extended,\r\n",
    "        })\r\n",
    "    metric.add_batch(predictions=true_predictions, references=true_labels)\r\n",
    "                                     \r\n",
    "                                     \r\n",
    "results = metric.compute()\r\n",
    "#printing the results\r\n",
    "print(\r\n",
    "    f\"epoch:\",\r\n",
    "    {\r\n",
    "        key: results[f\"overall_{key}\"]\r\n",
    "        for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\r\n",
    "    },\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}