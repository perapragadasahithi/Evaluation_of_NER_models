{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "from simpletransformers.ner import NERModel\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer, sent_tokenize\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import openpyxl\n",
    "from openpyxl import load_workbook\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "transformers_logger = logging.getLogger('transformers')\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from datasets import Features, Value, ClassLabel\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "import os\n",
    "\n",
    "#importing the conll file(test or train or validate)\n",
    "input_connl = '/host/Sahithi/myenv/an_validate_total.conll'\n",
    "\n",
    "#creating the dataset from the total annotated file\n",
    "def create_dataset_from_annotated_file(i):\n",
    "    input_file = i\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "        with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line :\n",
    "                if line.startswith('.'):\n",
    "                    word, _, _, label = line.split()\n",
    "                    current_sentence.append((word, label))\n",
    "                    if current_sentence:\n",
    "                        sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "                else:\n",
    "                    word, _, _, label = line.split()\n",
    "                    current_sentence.append((word, label))\n",
    "            else:\n",
    "\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence\n",
    "                current_sentence = []\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    c=0\n",
    "    conll_data = {\n",
    "\n",
    "        \"tokens\": [],\n",
    "        \"ner_tags\": []\n",
    "    }\n",
    "    s=0\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens, labels = zip(*sentence)\n",
    "        s+=1\n",
    "        c+=1\n",
    "        conll_data = {\n",
    "    \n",
    "        \"tokens\": [],\n",
    "        \"ner_tags\": []\n",
    "    }\n",
    "        conll_data[\"tokens\"] = [list(tokens)]\n",
    "        conll_data[\"ner_tags\"] = [convert_to_int_labels(list(labels), labels_list)[0]]\n",
    "        class_names = ['O', 'I-RRMS', 'I-Disease_Subtype', 'B-RRMS', 'I-SPMS', 'B-Disease_Subtype', 'B-EDSS', 'B-SPMS', 'B-Relapse_relative_date', 'I-Relapse_Date', 'I-EDSS', 'B-Relapse_date', 'B-EDSS_Value', 'I-Temporal_history_of_relapse', 'I-PPMS', 'B-Temporal_history_of_relapse', 'I-PRMS', 'B-MS_no_subtype', 'B-EDSS_Date', 'I-EDSS_Value', 'B-Relapse_Date', 'I-MS_no_subtype', 'B-Relapse_Temporality', 'I-Relapse_Temporality', 'B-PPMS', 'I-Relapse_date', 'I-CIS', 'I-Relapse_year', 'I-EDSS_Date', 'I-Relapse_relative_date', 'B-Relapse_year', 'B-CIS', 'B-PRMS', 'B-Severity_Severe', 'B-Relapse_Severity', 'B-Functional_Disability', 'B-Severity_Mild', 'I-Severity_Severe']\n",
    "        ft = Features({\n",
    "                       'tokens': datasets.Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "                       'ner_tags': datasets.Sequence(feature = ClassLabel(names=class_names), length = -1)})\n",
    "        if c==1:     \n",
    "            a = Dataset.from_dict(conll_data,features = ft)\n",
    "        else:\n",
    "            b = Dataset.from_dict(conll_data,features = ft)\n",
    "            assert b.features.type == a.features.type\n",
    "            a = concatenate_datasets([a,b])\n",
    "    return a\n",
    "                                     \n",
    "#conversion of string labels to integers\n",
    "def convert_to_int_labels(labels,labels_list):\n",
    "    label_to_int = {label: i for i, label in enumerate(labels_list)}\n",
    "    converted_ner_tags = []\n",
    "# Iterate through each example in the dataset and convert NER tags\n",
    "    converted_tags = [label_to_int[tag] for tag in labels]\n",
    "    converted_ner_tags.append(converted_tags)\n",
    "    return converted_ner_tags\n",
    " \n",
    "                                     \n",
    "#resultant dataset                                     \n",
    "test = create_dataset_from_annotated_file(input_connl)\n",
    " \n",
    "                                     \n",
    "#loading the model the model                                     \n",
    "from transformers import pipeline\n",
    "model_checkpoint = \"alvaroalon2/biobert_diseases_ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"ner\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "                                     \n",
    "                                     \n",
    "                                    \n",
    "ner_feature = test.features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names\n",
    "                                     \n",
    "                                     \n",
    "#aligning the labels with the tokens                                   \n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "                                     \n",
    "                                     \n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "                                     \n",
    "\n",
    "tokenized_datasets = test.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=test.column_names,\n",
    ")\n",
    "                                     \n",
    "                                     \n",
    "# Evaluation of the model over the complete dataset                                   \n",
    "import torch\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    labels = batch[\"labels\"]\n",
    "    # Necessary to pad predictions and labels for being gathered\n",
    "    predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "    predictions_gathered = accelerator.gather(predictions)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "    true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "                                     \n",
    "    token_names=[]\n",
    "    true_predictions_extended=[]\n",
    "    true_labels_extended=[]\n",
    "                                     \n",
    "    for ids in batch['input_ids']:\n",
    "        tokens=tokenizer.batch_decode(ids,skip_special_tokens=True)\n",
    "        token_names.extend(tokens)\n",
    "        for i in range(token_names.count('')):\n",
    "            token_names.remove('')\n",
    "    for i in true_predictions:\n",
    "        true_predictions_extended.extend(i)\n",
    "    for i in true_labels:\n",
    "        true_labels_extended.extend(i)                               \n",
    "    if(len(token_names)==len(true_labels_extended)):\n",
    "        total=pd.DataFrame({\n",
    "            \"token_name\":token_names,\n",
    "            \"true_label\":true_labels_extended,\n",
    "            \"true_prediction\":true_predictions_extended,\n",
    "        })\n",
    "    metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "                                     \n",
    "                                     \n",
    "results = metric.compute()\n",
    "#printing the results\n",
    "print(\n",
    "    f\"epoch:\",\n",
    "    {\n",
    "        key: results[f\"overall_{key}\"]\n",
    "        for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
